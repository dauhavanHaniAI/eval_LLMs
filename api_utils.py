import requests
import json
from transformers import AutoModelForCausalLM, AutoTokenizer


BASE_URL = "https://asko-llm.blaze.vn"
HEADERS = {"Content-Type": "application/json"}
MODEL = "" 


MODEL_NAME = "Qwen/Qwen3-8B"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

def call_asko_api(prompt: str, max_tokens: int = 100) -> str:
    """
    Sends a prompt to the LLM model deployed via API.

    Args:
        prompt (str): The input question or content.
        max_tokens (int): The maximum number of tokens to be generated by the model.

    Returns:
        str: The model's response content or an error message if any.
    """
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens
    }
    response = requests.post(f"{BASE_URL}/v1/chat/completions", headers=HEADERS, json=payload)
    try:
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Lá»—i: {e} | {response.status_code} | {response.text}"

def call_qwen(prompt: str, max_tokens: int = 100) -> str:
    """
    Generates text using the Qwen3 model.

    Args:
        prompt (str): The input question or content.
        max_tokens (int): The maximum number of tokens to be generated by the model.

    Returns:
        str: The output string generated by the model.
    """

    inputs = tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        max_length=max_tokens, 
                        truncation=True)

    outputs = model.generate(
                        **inputs, 
                        max_new_tokens=max_tokens, 
                        do_sample=False)
    outputs_token =  tokenizer.decode(outputs[0], skip_special_tokens=True)

    return outputs_token